# ğŸš€ Towards Ambiguity-Free Spatial Foundation Model  

[![ğŸ“„ arXiv](https://img.shields.io/badge/arXiv-2503.06014-red.svg)](https://arxiv.org/abs/2503.06014)  
[![ğŸ¥ Video Demo](https://img.shields.io/badge/Video%20Demo-Watch-blue.svg)](https://www.youtube.com/watch?v=38aSFah2jds)  
[![ğŸ“ License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)  
[![ğŸ› ï¸ Planned Code Release: May 2025](https://img.shields.io/badge/Code%20Release-May%202025-brightgreen.svg)]()  

---

## ğŸ” Introduction  

This repository provides resources for the paper **"Towards Ambiguity-Free Spatial Foundation Model: Rethinking and Decoupling Depth Ambiguity."**  

We tackle the challenge of **depth ambiguity** in spatial scene understanding, especially in **transparent and complex environments**. Traditional monocular depth models struggle with multi-layer depth perception. Our work introduces **Multi-Hypothesis Spatial Foundation Models (MH-SFMs)** to overcome these limitations, featuring:  

âœ… **MD-3k Benchmark** â€“ A dataset to evaluate multi-layer depth understanding and biases.  
âœ… **Laplacian Visual Prompting (LVP)** â€“ A training-free method to extract hidden depth from pre-trained models.  

Experiments confirm **LVP's effectiveness** for zero-shot multi-layer depth estimation and its advantages in downstream applications.  

<p align="center">
  <img src="./assets/pipeline.png" width="90%" alt="Towards Ambiguity-Free Multi-Hypothesis Spatial Foundation Model"/>
  <br>
  <em><b>ğŸ–¼ï¸ Figure 1. Motivation:</b> 3D spatial understanding, powered by (a) sensors and (b) algorithms, has been confined to a biased single-layer depth representation. (c) Existing methods collapse in complex 3D scenarios, particularly in ambiguous scenes like those with transparency. (d) We propose Laplacian Visual Prompting (LVP) to overcome this limitation, enabling Spatial Foundation Models to derive multi-hypothesis depth, unlocking ambiguity-free spatial understanding.</em>
</p>  

---

## ğŸ† Key Contributions  

ğŸ”¹ **MH-SFMs Paradigm** â€“ Reformulating depth estimation as multi-hypothesis inference to handle spatial ambiguity.  
ğŸ”¹ **MD-3k Benchmark** â€“ A novel dataset and evaluation metrics for multi-layer depth perception and model biases.  
ğŸ”¹ **Depth Bias Analysis** â€“ A comprehensive study of depth biases in existing models using MD-3k.  
ğŸ”¹ **Laplacian Visual Prompting (LVP)** â€“ A training-free method for multi-hypothesis depth estimation using pre-trained models.  
ğŸ”¹ **Extensive Validation** â€“ Demonstrating LVPâ€™s impact and advantages across various downstream applications.  

---

## ğŸš€ Getting Started (Code Release: **May 2025**)  

ğŸ—“ï¸ **Planned Release**: May 2025. Stay tuned for:  

ğŸ“‚ **MD-3k Dataset** â€“ Instructions for downloading and using our benchmark dataset.  
ğŸ’¡ **LVP Implementation** â€“ Code for running Laplacian Visual Prompting on pre-trained models.  
ğŸ“‘ **Reproducibility Guide** â€“ Example scripts for reproducing our experiments.  

---

ğŸ“© **Stay Updated!**  
For updates, please â­ star this repo and check back for the official release.  
